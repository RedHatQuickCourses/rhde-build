:time_estimate: 5

= Design, Build, and Publish Edge Images

_Estimated reading time: *{time_estimate} minutes*._

Objective::

Understand the characteristics of RHEL for Edge images and the workflows for building and publishing them.

== The RHEL Image Builder Service and Clients

The Image Builder tools is a system service in RHEL, named *osbuild-composer*, with which you interact by means of the *composer-cli* command-line client or the *Composer* Cockpit module. It can generate various types of system images, including edge images.

[ figure of image builder service, its clients, input blueprint, and output images ]

The Image Builder service in RHEL is not designed for remote clients, and it does not support creating system images, or *composes*, for operating systems different than the one it is running on. For example, you cannot use the Image Builder service from RHEL 9 to build a RHEL 8 nor a CentOS 9 Stream system image. Most likely, you will be running Image Builder in a VM and using SSH or Cockpit to interact with Image Builder on that VM.

You provide to Image Builder a blueprint using the https://toml.io/en/[TOML] syntax, which is very similar to INI files from Windows and Samba. A blueprint defines characteriscs of the system image, such as installed packages and enabled system services. It also enables customization of users, networking, and other system configurations, and not all of them are supported for edge images. You can think of a blueprint as a response file for the prompts and options you would get from the RHEL installation program, Anaconda.

An Image Builder blueprint should be independent of the target environment where you deploy the image. When you build a compose, Image Builder adds hardcoded customizations which depend on the image type. For example, if you build an Amazon Web Services Machine Image (AMI), Image Builder enables the Cloud-Init service and also adds AWS agents. You could use the same blueprint to build different types of composes, for example ISO images to boot a physical system and Qcow2 images for OpenStack private clouds.

The biggest benefit of using Image Builder, compared to generic Anaconda or Ansible automation, is the built-in knowledge about the needs of different target platforms such as cloud providers and hypevisors. It ensures your image contains all required and recommended settings and system services for working whell on those platforms.

All blueprints and composes are kept in the Image Builder service storage, and you must upload blueprints to build composes, download blueprints to make changes, and download composes so you can publish them somewhere suitable as a boot source for your physical or virtual systems. As a convenience, Image Builder can upload images by itself to a limited set of remote image stores. For example, it can take Amazon Web Services credentials as input and upload AMI images to AWS S3 so you can use them to create AWS instances.

Depending on the target environment and the type of image, you can use different methods to perform additional installation-time (or day-1) customization of those images, for example Anaconda Kickstart files and Cloud-init. Or you could build your own workflow for adding local customizations to an image, for example by using an interactive application which runs during the first boot.

== Edge Images and RPM-OSTree

Regular compose types from Image Builder work the same as if you deploy RHEL from its installation media: they are package-based systems, which you can manage by adding and updating individual packages and changing files anywhere in the file system. 

Edge images, in contrast, assume most customizations are predefined in the system image, and that software updates are performed by installing a new, complete system image. For example, if your system image includes the Apache Web Server, and you want to update to mitigate a CVE vulnerability, you would not update the httpd-server package on edge devices, but you would build a new system image which includes the updated package and then update your edge devices to use the new system image.

Edge images from Image Builder are based on the OSTree technology, which configures all system files as read-only. You cannot update a system file, not even as root, except by deploying a new system image an rebooting into the new image. OSTree can keep multiple system images side-by-side in a system, so downloading updates do not affect the running system -- constrast that with RPM package updates, which can break running processes. And you can reboot into any of the available system images, making it trivial to rollback a system update to a previous known good image -- constrast that with rolling back an update of multiple RPM packages. With package-based systems, you can select an older kernel at boot, but you cannot easily revert all system binaries which depend on the kernel version.

OSTree allows writes to only `/etc` and `/var`, which are treated in different ways:

*  Files in `/var` are only touched at first boot, when they're copied from the system image, but updates keep the contents in `/var` as they are. Applications must be designed to migrate and convert data to new formats, if required.

* Files in `/etc` are updated with a three-way merge process: the current `/etc` contents are compared with the original system image, and differences are applied to the contents of the `/etc` contents from the new system image. This way, new system images can bring new configuration files ot change configuration defaults, while preserving local customizations applied to the running system.

[ figure of read-only /usr and read-write /etc and /var. Also /sysroot? ]

The OSTree technology is not designed to be used by itself, but to be infrastructure bellow higher-level Linux distribution tooling. In RHEL, the higher level infrastructure is RPM-OSTree, which enables building OSTree system images from RPM packages (like Anaconda builds a root file system during installation). RPM-OSTree also enables managing a system deployed from an image-based workflow as if it were a package-based system, by creating a layer of local customizations, which you can either keep or discard during system updates. It is not expected that edge systems abuse that feature of RPM-OSTree.

Image Builder users do not interact with the RPM-OSTree tools directly, at least not to build and udpdate edge images. But they must interact with RPM-OSTree tools to distribute and apply updates to edge systems.

== Edge Image Types

The only indispensable type of edge image (or edge compose) from Image Builder is the *commit* image. It provides an OSTree image, called a *OSTree commit*, in raw format as a single, compressed archive.

That OSTree commit can be pushed into an OSTree repository, and then consumed by the regular RHEL installation image (or the much smaller RHEL network installer) with a custom Kickstart file, to deploy into a physical system or virtual machine. This is a viable workflow for developer testing and for CI/CD systems.

You can also use Image Builder to take an OSTree commit and build either an *edge installer* image or a *edge cloud* image. Those composer types enable creating bootable media which includes a RHEL installer and an OSTree commit, so you can install an edge system without remote access to an OSTree repository.

[ figure of RHEL ISO + remote OSTree commit and edge installer image with embeded OSTree commit ]

You could use either the standard RHEL installation image or an edge installer image to perform network boot and install edge systems without booting from local media.

Whatever the boot method and image type you use for installing an edge system, they require access to remote OSTree repositories to fetch and deploy system updates. You cannot build an "edge update image", which would boot from local media or network boot and apply a system update. Such feature is currently in development, as part of the Red Hat In Veichle Operating System (RHIVOS), but it is not supported yet by Image Builder on RHEL.

In fact, all bootable types of edge composers supported by Image Builder are preconfigured with an OSTree remote, which points to an OSTree repository server, except for the *edge container* image. This is a special type of composer which serves an OSTree commit over HTTP. It is intended as a quick way for a developer to provision a remote OSTree repository to test edge installer images, but it is not designed for long term usage nor for serving system updates.

In this course, you will NOT use edge container composes, you will instead configure and manage remote OSTree repositories so you learn how to provide system updates to real edge devices.

== Publishng OSTree Commits

OSTree is, conceptually, very similar to Git: both are designed to manage changes to file system trees as atomic units, and to provide transactional updates and rollbacks to those trees. Both are also designed to manage multiple concurrent branches and to be efficient on both disk space and network bandwidth when handling deltas between two versions (or two commits) of the same file system tree.

Why not just using Git? OSTree adds the following features, which are required to manage bootable operating system trees but are not needed to manage application source code:

* Recording SELinux labels and POSIX extended attributes
* Installing booloaders, Linux Kernels, and initial ramdisks

Similar to Git, a client OSTree system contains a copy (or a clone) of a remote OSTree repository, and it can pull and push changes from the remote. Unlike Git, OSTree is optimized for handling large binary files, as opposed to small text files.

Also unlike Git, OSTree is designed to discard history, on the assumption that all operating system binaries in a commit can be recreated from their source code. An OSTree repository is not intended for long-term storage and auditing, not for tracking changes: these are best done on the source code rather than on the derived binaries.

The most striking difference between OSTree and Git is the fact that OSTree provides no specialized server and client software: OSTree repositories are just files serverd by a standard HTTP server. You manage OSTree repositories using local file system access, and remote access over HTTP works only for downloading (pulling) updates from a remote to a local repository.

While this means that managing and updating OSTree repositories requires more effort than with Git, it also means you don't need to learn how to deploy and manage special server software: you just need to learn how to use the client-side OSTree tools, or just the client tools from a higher-level abstraction such as RPM-OSTree.

== OSTree Static Deltas

[ Move this heading to the update topic? ]

OSTree repositories can include OSTree commits with no history between them, and yet be able to deduplicate file contents on local and remote repositories, and also optimize network traffic by sending only deltas from local to remote repository. So you can build many OSTree commits in a development system, using its local repository, but only promote for production usage, in a remote repository, the commits which passed quality assurance tests.

But, as Git users know, computing deltas and transfering them in a file-by-file basis is not efficient, requiring multiple network transactions and lots of protocol overhead. OSTree offers the possibility of precomputing and storing static deltas between two commits as part of an OSTree repository. OSTree clients will discover and use the deltas transparently.

Having precomputed static deltas also reduces the memory and CPU requirements of both clients and servers when updating systems. If you consider that an OSTree remote is serving content to a potentially larger population of edge devices, compared to the population of developers using a Git repository, the efficiency gains can be huge, not to mention a lower cloud bill for running your OSTree remotes.

Precomputed static deltas also saves CPU from smaller edge devices while downloading and deploy system updates. This is significant because those devices are expected to contine performing their regular tasks while downloading (or staging) system updates.

== The Composer CLI client

Lorem ipsum

== Next Steps

Lorem Ipsum